{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2hYywfC5Cl9"
   },
   "source": [
    "# Clasificación por RNA de datos generados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlZbXRokvjqN"
   },
   "source": [
    "## Caso de dos clases, una asociada a una mezcla de gaussianas multivariadas y otra con una gaussiana multivariada. **Matriz de covarianza igual**\n",
    "\n",
    "En este código empleamos redes neuronales para clasificar datos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2vKYQu37m5n"
   },
   "source": [
    "### Generación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBc1D6aXGc68"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "# Generación de datos aleatorios usando la mezcla de gaussianas\n",
    "# Parámetros de la mezcla de 3 gaussianas\n",
    "n = 5000 #Tamaño de la muestra\n",
    "\n",
    "# definimos la matriz de medias (la fila k es el vector de medias de la k-ésima normal)\n",
    "means = np.array([[-3, 0],[ 2.5, 5],[8,-0.5]])\n",
    "#Matriz de covarianza\n",
    "cov2=np.array([[8,0.5],[0.5,3]])\n",
    "\n",
    "weights = [0.4, 0.2, 0.4]  # Pesos de las tres distribuciones (deben sumar 1)\n",
    "\n",
    "#Fijamos la semilla\n",
    "np.random.seed(1)\n",
    "\n",
    "# Elegir qué componente generar para cada muestra\n",
    "componentes = np.random.choice([0, 1, 2], size=n, p=weights)\n",
    "# Aquí lo que hacemos es generar un vector que solo conste de 0,1 y 2's, cada uno con cierta probabilidad de aparecer (pero con elección aleatoria)\n",
    "# Cada entrada j determina la distribución de la normal que debe tener el dato j\n",
    "\n",
    "# Generamos los datos aleatorios de la clase 0\n",
    "datos0 = np.array([np.random.multivariate_normal(means[i], cov2) for i in componentes])\n",
    "\n",
    "# Separamos las coordenadas x e y para graficar\n",
    "x0=datos0[:,0]\n",
    "y0=datos0[:,1]\n",
    "\n",
    "#Generación de datos de la gaussiana multivariada\n",
    "# Vector de medias para x,y\n",
    "mean1 = [0, -6]\n",
    "# Matriz de covarianza consideramos la misma\n",
    "\n",
    "#Generamos los datos aleatorios de la clase 1\n",
    "datos1 = np.random.multivariate_normal(mean1, cov2, n)\n",
    "\n",
    "# Separar las coordenadas x e y\n",
    "x1, y1 = datos1.T  # Transponer los datos para separar las dos dimensiones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1gMrE7hsJHt"
   },
   "source": [
    "Etiquetamos nuestro datos en clases para usar estos datos fijos como modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj4cpYeLsSne"
   },
   "outputs": [],
   "source": [
    "#Clase 1\n",
    "clas0=np.transpose(np.array(np.zeros(n)))\n",
    "#Clase 2\n",
    "clas1=np.transpose(np.array(np.ones(n)))\n",
    "Clas=np.concatenate((clas0,clas1))\n",
    "Clas=np.expand_dims(Clas, axis=0)  # Necesitamos un vector, por ello agregamos una dimensión al arreglo\n",
    "\n",
    "Training_data=np.transpose(np.concatenate((datos0,datos1))) #Matriz de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbr239gn5eg7"
   },
   "source": [
    "Generamos de igual manera los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ7vXjBM5iZE"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) #Fijamos una semilla para los datos de prueba\n",
    "m=5000 #Número de datos de prueba\n",
    "#Se repite la generación de datos aleatorios del inicio\n",
    "pesos = np.random.choice([0, 1, 2], size=m, p=weights)\n",
    "test0=np.array([np.random.multivariate_normal(means[i], cov2) for i in pesos])\n",
    "test1 = np.random.multivariate_normal(mean1, cov2, m)\n",
    "#Juntamos los datos de prueba\n",
    "Test=np.transpose(np.concatenate((test0,test1)))\n",
    "TestX=np.expand_dims(Test[0],axis=1)\n",
    "TestY=np.expand_dims(Test[1],axis=1)\n",
    "\n",
    "#Les asignamos las etiquetas\n",
    "EtiqClas0=np.transpose(np.array(np.zeros(m)))\n",
    "EtiqClas1=np.transpose(np.array(np.ones(m)))\n",
    "Etiquetas=np.concatenate((EtiqClas0,EtiqClas1))\n",
    "Etiquetas=np.expand_dims(Etiquetas, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAUaXUFUmUpr"
   },
   "source": [
    "Malla sobre la cual dibujaremos nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMlJUK_hzDPZ"
   },
   "outputs": [],
   "source": [
    "X=np.linspace(-15,15,100) #Espaciado en x\n",
    "Y=np.linspace(-12,12,100) #Espaciado en y\n",
    "Mx, My = np.meshgrid(X,Y)  # Malla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF6AmTwCojoe"
   },
   "source": [
    "Consideramos construir las probabilidades a posteriori para este caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaJvo4Q6oqEY"
   },
   "outputs": [],
   "source": [
    "#Determinamos la funcion de densidad de la mezcla gaussiana\n",
    "def Np(x,y):\n",
    "  data=np.dstack((x,y))\n",
    "  dim1,dim2=x.shape\n",
    "  B=np.zeros((dim1,dim2))\n",
    "  for i in range(len(weights)):\n",
    "    rv=stats.multivariate_normal(means[i],cov2)\n",
    "    z=rv.pdf(data)\n",
    "    z=weights[i]*z\n",
    "    C=z.copy()\n",
    "    B=B+C\n",
    "  return B\n",
    "\n",
    "def OnlyNp(x,y):\n",
    "  data=np.dstack((x,y))\n",
    "  dim1,dim2=x.shape\n",
    "  B=np.zeros((dim1,dim2))\n",
    "  rv=stats.multivariate_normal(mean1,cov2)\n",
    "  z=rv.pdf(data)\n",
    "  return z\n",
    "\n",
    "#Definimos el clasificador por la regla de Bayes. En este caso estamos calculando la probabilidad P(Pi_2|x)\n",
    "def Bayes(Mx,My):\n",
    "  W=OnlyNp(Mx,My)\n",
    "  Z=Np(Mx,My)\n",
    "  P=W/(Z+W)\n",
    "  return P\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY2EFHmW69gW"
   },
   "source": [
    "### Graficando puntos\n",
    "\n",
    "En esta sección graficamos nuestros datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywbCdJIW7MJl"
   },
   "outputs": [],
   "source": [
    "#Determinamos la superficie generada por la probabilidad a posteriori\n",
    "P=Bayes(Mx,My)\n",
    "Z=Np(Mx,My)\n",
    "W=OnlyNp(Mx,My)\n",
    "\n",
    "# Graficamos los puntos\n",
    "# Crear la figura y el objeto Axes3D\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "cnt=plt.contour(Mx,My,P, levels=1,colors='black')\n",
    "plt.scatter(x1, y1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.scatter(x0, y0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.title('Datos de entrenamiento y regla de Bayes')\n",
    "plt.clabel(cnt, cnt.levels, inline = True, fontsize = 10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('mi_grafico1.png', dpi=300, bbox_inches='tight')\n",
    "files.download('mi_grafico1.png')\n",
    "plt.show()\n",
    "#Gráfico para entender como se comportan las normales\n",
    "plt.contour(Mx,My,Z, levels=50, colors='teal')\n",
    "plt.contour(Mx,My,W, levels=50, colors='orange')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2wTtVHUpOcS"
   },
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Entrenamiento de nuestra red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOiOr-ispNvt"
   },
   "outputs": [],
   "source": [
    "def shallow_NN( X, Y, lr, epochs, nh1):\n",
    "  # Numero de observaciones\n",
    "  m = X.shape[1]\n",
    "\n",
    "  # Layers: input, two hidden\n",
    "  n0 = X.shape[0]\n",
    "  n1 = nh1\n",
    "  n2 = 1\n",
    "\n",
    "  # Condiciones iniciales\n",
    "  A0 = X\n",
    "  W1 = np.random.randn(n1 ,n0) * 0.01\n",
    "  b1 = np.zeros((n1,1))\n",
    "  #print(A0.shape, W1.shape, b1.shape)\n",
    "\n",
    "  W2 = np.random.randn(n2 ,n1) * 0.01\n",
    "  b2 = np.zeros((n2,1))\n",
    "\n",
    "  loss = []\n",
    "  for i in range(epochs):\n",
    "\n",
    "    # Forward propagation\n",
    "    Z1 = W1 @ A0 + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = ( 1.0 + np.exp(-Z2) )**(-1)\n",
    "\n",
    "    # Backward propagation\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1.0/m) * ( dZ2 @ A1.T )\n",
    "    db2 = (1.0/m) * np.sum( dZ2, axis=1, keepdims=True )\n",
    "\n",
    "\n",
    "    dZ1 =  (W2.T @ dZ2) * ( 1.0 - A1**2)\n",
    "    dW1 = (1.0/m) * ( dZ1 @ A0.T )\n",
    "    db1 = (1.0/m) * np.sum( dZ1, axis=1, keepdims=True )\n",
    "\n",
    "\n",
    "    # Actualización de los pesos\n",
    "    W1 = W1 - lr*dW1\n",
    "    b1 = b1 - lr*db1\n",
    "\n",
    "    W2 = W2 - lr*dW2\n",
    "    b2 = b2 - lr*db2\n",
    "\n",
    "    loss.append( -(1.0/m)*np.sum( Y*np.log(A2) + (1.0-Y)*np.log( 1.0 - A2)) )\n",
    "    #print(i, loss[-1])\n",
    "  print(f'La función de costo en la iteración {epochs} es {loss[-1]}')\n",
    "  return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'loss':loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H5tqCYFwx19"
   },
   "source": [
    "Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAPRVdDyw0LZ"
   },
   "outputs": [],
   "source": [
    "nh1=1000\n",
    "result=shallow_NN( Training_data, Clas, 1, 200, nh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T41whHgn6UuT"
   },
   "source": [
    "Creamos la evaluación de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnbMB2Hf5arf"
   },
   "outputs": [],
   "source": [
    "plt.plot(result['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0WOd7WH-I0H"
   },
   "source": [
    "### Predicciones de los datos de prueba\n",
    "Generamos las predicciones de nuestros datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TxQ93ZY_xwq"
   },
   "outputs": [],
   "source": [
    "def predict( results, X ):\n",
    "\n",
    "  A0 = X\n",
    "  W1 = results['W1']\n",
    "  b1 = results['b1']\n",
    "\n",
    "  W2 = results['W2']\n",
    "  b2 = results['b2']\n",
    "\n",
    "  # Forward propagation\n",
    "  Z1 = W1 @ A0 + b1\n",
    "  A1 = np.tanh(Z1)\n",
    "\n",
    "  Z2 = W2 @ A1 + b2\n",
    "  A2 = ( 1.0 + np.exp(-Z2) )**(-1)\n",
    "\n",
    "  #yhat = np.around(A2)\n",
    "  return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OQAXn2TGl2K"
   },
   "source": [
    "Hacemos las predicciones de nuestro datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsGRwZ7vcng4"
   },
   "outputs": [],
   "source": [
    "y_test_hat = np.around(predict(result, Test))\n",
    "Bayes_test_hat=np.around(Bayes(np.transpose(TestX),np.transpose(TestY)))\n",
    "print(y_test_hat)\n",
    "print(Bayes_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VgqyZB826Tb"
   },
   "source": [
    "Calculamos el porcentaje de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KDDdCvB25sV"
   },
   "outputs": [],
   "source": [
    "#Vector de errores para ambos clasificadores (si la diferencia es 0, entonces el valor se está clasificando correctamente, en caso contrario dará 1,-1)\n",
    "ERROR_NN=np.abs(y_test_hat-Etiquetas)\n",
    "ERROR_BAYES=np.abs(Bayes_test_hat-Etiquetas)\n",
    "#Cuantificamos la cantidad de Errores cometidos entre la cantidad de datos de prueba 2m\n",
    "print(f'La proporción de error de clasificación es {np.sum(ERROR_NN)/(2*m)} para la red neuronal')\n",
    "print(f'La proporción de error de clasificación es {np.sum(ERROR_BAYES)/(2*m)} para la regla de Bayes')\n",
    "#Calculamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJIIl428mIM6"
   },
   "source": [
    "### Graficando el clasificador generado por la red neuronal\n",
    "Graficamos los puntos pruebas y como fueron clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3jTFzkcm_bg"
   },
   "outputs": [],
   "source": [
    "# Graficamos los puntos\n",
    "\n",
    "#Primero separamos los datos como los clasifico la red neuronal\n",
    "x_T0=Test[0,y_test_hat[0,:]==0]\n",
    "y_T0=Test[1,y_test_hat[0,:]==0]\n",
    "x_T1=Test[0,y_test_hat[0,:]==1]\n",
    "y_T1=Test[1,y_test_hat[0,:]==1]\n",
    "\n",
    "#Los datos clasificados realmente\n",
    "x_R0=test0[:,0]\n",
    "y_R0=test0[:,1]\n",
    "x_R1=test1[:,0]\n",
    "y_R1=test1[:,1]\n",
    "\n",
    "#Generamos la regla de clasificación dada por la red neuronal\n",
    "Q=np.array((Mx,My)) #2x100x100\n",
    "A=np.zeros((100,100))\n",
    "#Generamos la información de la superficie generada por la red sobre la malla\n",
    "for i in range(100):\n",
    "  for j in range(100):\n",
    "    S=np.expand_dims(Q[:,i,j],axis=1)\n",
    "    A[i,j]=predict(result, S)\n",
    "\n",
    "#Cortamos para obtener la curva de nivel 0.5\n",
    "cn2=plt.contour(Mx,My,A, levels=1, colors='red', linestyles= \"dashed\")\n",
    "plt.clabel(cn2, cn2.levels, inline = True, fontsize = 8)\n",
    "\n",
    "# Graficamos los datos clasificados según la red\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "plt.scatter(x_T0, y_T0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.scatter(x_T1, y_T1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.title(f'Datos clasificados por la red con {nh1} neuronas')\n",
    "\n",
    "#Función de separación optima obtenida mediante la distribución de los datos\n",
    "cnt=plt.contour(Mx,My,P, levels=1, colors='black')\n",
    "plt.clabel(cnt, cnt.levels, inline = True, fontsize = 8)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Figura de los datos aleatorios con su etiqueta real\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "plt.scatter(x_R0, y_R0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.scatter(x_R1, y_R1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.title(f'Clasificador por una red con {nh1} neuronas ocultas')\n",
    "\n",
    "#Función de separación obtenida mediante la red\n",
    "cn2=plt.contour(Mx,My,A, levels=1, colors='red', linestyles= \"dashed\")\n",
    "plt.clabel(cnt, cnt.levels, inline = False, fontsize = 8)\n",
    "#Función de separación optima obtenida mediante la distribución de los datos\n",
    "cnt=plt.contour(Mx,My,P, levels=1, colors='black')\n",
    "plt.clabel(cn2, cn2.levels, inline = False, fontsize = 8)\n",
    "plt.annotate('Clasificador de Bayes', (-11,-5.1), fontsize=6, color='black')\n",
    "plt.annotate('Clasificador de la Red', (-11,-2), fontsize=6, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('mi_grafico1.png', dpi=300, bbox_inches='tight')\n",
    "files.download('mi_grafico1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUJtny2GCF9p"
   },
   "source": [
    "### Comparación de resultados cambiando el número de neuronas\n",
    "\n",
    "En esta sección ejecutamos todos los códigos anteriores variando las neuronas en la capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPC4mE0MGnN1"
   },
   "outputs": [],
   "source": [
    "#Damos un vector donde cada entrada es el número de neuronas en la capa\n",
    "Neurons=[1,2,3,4,5,10,20,50,100,200]\n",
    "\n",
    "#Ejecutamos para cada cantidad de neuronas\n",
    "for t in Neurons:\n",
    "  result=shallow_NN( Training_data, Clas, 1, 500, nh1=t)\n",
    "  #Proporción de error\n",
    "  y_test_hat = np.around(predict(result, Test))\n",
    "  #Vector de errores para ambos clasificadores (si la diferencia es 0, entonces el valor se está clasificando correctamente, en caso contrario dará 1,-1)\n",
    "  ERROR_NN=np.abs(y_test_hat-Etiquetas)\n",
    "  ERROR_BAYES=np.abs(Bayes_test_hat-Etiquetas)\n",
    "  #Cuantificamos la cantidad de Errores cometidos entre la cantidad de datos de prueba 2m\n",
    "  print(f'La proporción de error de clasificación es {np.sum(ERROR_NN)/(2*m)} para la red neuronal')\n",
    "  print(f'La proporción de error de clasificación es {np.sum(ERROR_BAYES)/(2*m)} para la regla de Bayes')\n",
    "  #Graficamos los datos reales con su etiqueta real\n",
    "  plt.xlim(-12,12)\n",
    "  plt.ylim(-12,12)\n",
    "  plt.scatter(x_R0, y_R0, alpha=0.5, color='black', label='Clase 0')\n",
    "  plt.scatter(x_R1, y_R1, alpha=0.5, color='red', label='Clase 1')\n",
    "  #Por cada iteración calculamos la regla de decisión de la red neuronal\n",
    "  Q=np.array((Mx,My)) #2x100x100\n",
    "  A=np.zeros((100,100))\n",
    "  #Graficamos la superficie\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      S=np.expand_dims(Q[:,i,j],axis=1)\n",
    "      A[i,j]=predict(result, S)\n",
    "\n",
    "  #Cortamos para obtener la curva de nivel 0.5\n",
    "  cn2=plt.contour(Mx,My,A, levels=1, colors='green', linestyles= \"dashed\")\n",
    "  plt.clabel(cn2, cn2.levels, inline = False, fontsize = 8)\n",
    "  plt.title(f'Regla de decisión con {t} neuronas')\n",
    "\n",
    "  cnt=plt.contour(Mx,My,P, levels=1)\n",
    "  plt.clabel(cnt, cnt.levels, inline = False, fontsize = 8)\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "  plt.annotate('Clasificador de Bayes', (-11,-5.1), fontsize=6, color='teal')\n",
    "  plt.annotate('Clasificador de la Red', (-11,-2), fontsize=6, color='green')\n",
    "\n",
    "\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_KoBVeltofX"
   },
   "source": [
    "## Caso de dos clases, una asociada a una mezcla de gaussianas multivariadas y otra con una gaussiana multivariada. **Matrices de covarianzas no necesariamente iguales**.\n",
    "\n",
    "En este código empleamos redes neuronales para clasificar datos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbmRVhJft575"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "# Generación de datos aleatorios usando la mezcla de gaussianas\n",
    "# Parámetros de la mezcla de 3 gaussianas\n",
    "n = 5000 #Tamaño de la muestra\n",
    "\n",
    "# definimos la matriz de medias (la fila k es el vector de medias de la k-ésima normal)\n",
    "means = np.array([[-3, 0],[ 2.5, 5],[3,-6]])\n",
    "#Matrices de covarianza\n",
    "cov1=np.array([[6,0.3],[0.3,5]])\n",
    "cov2=np.array([[5,0.5],[0.5,2]])\n",
    "cov3=np.array([[4,0.5],[0.5,2]])\n",
    "\n",
    "cov=np.array([[1,0],[0,2]])\n",
    "\n",
    "# Vector de desviaciones estándar de las tres distribuciones\n",
    "std_devs = np.array([cov1,cov2, cov3])\n",
    "weights = [0.3,0.4,0.3]  # Pesos de las tres distribuciones (deben sumar 1)\n",
    "#Fijamos la semilla\n",
    "np.random.seed(1)\n",
    "\n",
    "# Elegir qué componente generar para cada muestra\n",
    "componentes = np.random.choice(range(len(weights)), size=n, p=weights)\n",
    "# Aquí lo que hacemos es generar un vector que solo conste de 0,1 y 2's, cada uno con cierta probabilidad de aparecer (pero con elección aleatoria)\n",
    "# Cada entrada j determina la distribución de la normal que debe tener el dato j\n",
    "\n",
    "# Generamos los datos aleatorios de la clase 0\n",
    "datos0 = np.array([np.random.multivariate_normal(means[i], std_devs[i]) for i in componentes])\n",
    "\n",
    "# Separamos las coordenadas x e y para graficar\n",
    "x0=datos0[:,0]\n",
    "y0=datos0[:,1]\n",
    "\n",
    "#Generación de datos de la gaussiana multivariada\n",
    "# Vector de medias para x,y\n",
    "mean1 = [7,-1]\n",
    "# Matriz de covarianza consideramos la misma\n",
    "\n",
    "#Generamos los datos aleatorios de la clase 1\n",
    "datos1 = np.random.multivariate_normal(mean1, cov, n)\n",
    "\n",
    "# Separar las coordenadas x e y\n",
    "x1, y1 = datos1.T  # Transponer los datos para separar las dos dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_wBvtCluikY"
   },
   "source": [
    "Etiquetamos nuestros datos y generamos nuestros datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rrfvFD5ukMr"
   },
   "outputs": [],
   "source": [
    "#Clase 1\n",
    "clas0=np.transpose(np.array(np.zeros(n)))\n",
    "#Clase 2\n",
    "clas1=np.transpose(np.array(np.ones(n)))\n",
    "Clas=np.concatenate((clas0,clas1))\n",
    "Clas=np.expand_dims(Clas, axis=0)  # Necesitamos un vector, por ello agregamos una dimensión al arreglo\n",
    "\n",
    "Training_data=np.transpose(np.concatenate((datos0,datos1))) #Matriz de datos de entrenamiento\n",
    "\n",
    "np.random.seed(3) #Fijamos una semilla para los datos de prueba\n",
    "m=5000 #Número de datos de prueba\n",
    "#Se repite la generación de datos aleatorios del inicio\n",
    "pesos = np.random.choice(range(len(weights)), size=m, p=weights)\n",
    "test0=np.array([np.random.multivariate_normal(means[i], std_devs[i]) for i in pesos])\n",
    "test1 = np.random.multivariate_normal(mean1, cov, m)\n",
    "#Juntamos los datos de prueba\n",
    "Test=np.transpose(np.concatenate((test0,test1)))\n",
    "TestX=np.expand_dims(Test[0],axis=1)\n",
    "TestY=np.expand_dims(Test[1],axis=1)\n",
    "\n",
    "\n",
    "EtiqClas0=np.transpose(np.array(np.zeros(m)))\n",
    "EtiqClas1=np.transpose(np.array(np.ones(m)))\n",
    "Etiquetas=np.concatenate((EtiqClas0,EtiqClas1))\n",
    "Etiquetas=np.expand_dims(Etiquetas, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tDZItqLu6BF"
   },
   "source": [
    "Generamos nuestra malla donde graficaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfOh9JDPu5za"
   },
   "outputs": [],
   "source": [
    "X=np.linspace(-15,15,100) #Espaciado en x\n",
    "Y=np.linspace(-12,12,100) #Espaciado en y\n",
    "Mx, My = np.meshgrid(X,Y)  # Malla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ8dpzp1vK1H"
   },
   "source": [
    "Calculamos la regla de Bayes para nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlseLiNdvKrw"
   },
   "outputs": [],
   "source": [
    "#Determinamos la funcion de densidad de la mezcla gaussiana\n",
    "def Np(x,y):\n",
    "  data=np.dstack((x,y))\n",
    "  dim1,dim2=x.shape\n",
    "  B=np.zeros((dim1,dim2))\n",
    "  for i in range(len(weights)):\n",
    "    rv=stats.multivariate_normal(means[i],std_devs[i])\n",
    "    z=rv.pdf(data)\n",
    "    z=weights[i]*z\n",
    "    C=z.copy()\n",
    "    B=B+C\n",
    "  return B\n",
    "\n",
    "def OnlyNp(x,y):\n",
    "  data=np.dstack((x,y))\n",
    "  dim1,dim2=x.shape\n",
    "  B=np.zeros((dim1,dim2))\n",
    "  rv=stats.multivariate_normal(mean1,cov)\n",
    "  z=rv.pdf(data)\n",
    "  return z\n",
    "\n",
    "#Definimos el clasificador por la regla de Bayes. En este caso estamos calculando la probabilidad P(Pi_2|x)\n",
    "def Bayes(Mx,My):\n",
    "  W=OnlyNp(Mx,My)\n",
    "  Z=Np(Mx,My)\n",
    "  P=W/(Z+W)\n",
    "  return P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXpCUy-uvWV2"
   },
   "source": [
    "### Graficamos nuestros puntos\n",
    "\n",
    "En esta sección mostramos nuestros datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ry9lw8NmvcUb"
   },
   "outputs": [],
   "source": [
    "#Determinamos la superficie generada por la probabilidad a posteriori\n",
    "P=Bayes(Mx,My)\n",
    "Z=Np(Mx,My)\n",
    "W=OnlyNp(Mx,My)\n",
    "\n",
    "# Graficamos los puntos\n",
    "# Crear la figura y el objeto Axes3D\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "cnt=plt.contour(X,Y,P, levels=1, colors='black')\n",
    "plt.scatter(x1, y1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.scatter(x0, y0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.title('Datos de entrenamiento y regla de Bayes')\n",
    "plt.clabel(cnt, cnt.levels, inline = True, fontsize = 10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('mi_grafico1.png', dpi=300, bbox_inches='tight')\n",
    "files.download('mi_grafico1.png')\n",
    "plt.show()\n",
    "#Gráfico para entender como se comportan las normales\n",
    "plt.contour(Mx,My,Z, levels=50, colors='teal')\n",
    "plt.contour(Mx,My,W, levels=50, colors='orange')\n",
    "plt.show()\n",
    "\n",
    "plt.contourf(Mx,My,P, levels=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwwN-3V8uYyw"
   },
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "En esta sección ejecutamos el entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvUV-5jPuf-T"
   },
   "outputs": [],
   "source": [
    "def shallow_NN( X, Y, lr, epochs, nh1):\n",
    "  # Numero de observaciones\n",
    "  m = X.shape[1]\n",
    "\n",
    "  # Layers: input, two hidden\n",
    "  n0 = X.shape[0]\n",
    "  n1 = nh1\n",
    "  n2 = 1\n",
    "\n",
    "  # Condiciones iniciales\n",
    "  A0 = X\n",
    "  W1 = np.random.randn(n1 ,n0) * 0.01\n",
    "  b1 = np.zeros((n1,1))\n",
    "  #print(A0.shape, W1.shape, b1.shape)\n",
    "\n",
    "  W2 = np.random.randn(n2 ,n1) * 0.01\n",
    "  b2 = np.zeros((n2,1))\n",
    "\n",
    "  loss = []\n",
    "  for i in range(epochs):\n",
    "\n",
    "    # Forward propagation\n",
    "    Z1 = W1 @ A0 + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = ( 1.0 + np.exp(-Z2) )**(-1)\n",
    "\n",
    "    # Backward propagation\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1.0/m) * ( dZ2 @ A1.T )\n",
    "    db2 = (1.0/m) * np.sum( dZ2, axis=1, keepdims=True )\n",
    "\n",
    "\n",
    "    dZ1 =  (W2.T @ dZ2) * ( 1.0 - A1**2)\n",
    "    dW1 = (1.0/m) * ( dZ1 @ A0.T )\n",
    "    db1 = (1.0/m) * np.sum( dZ1, axis=1, keepdims=True )\n",
    "\n",
    "\n",
    "    # Actualización de los pesos\n",
    "    W1 = W1 - lr*dW1\n",
    "    b1 = b1 - lr*db1\n",
    "\n",
    "    W2 = W2 - lr*dW2\n",
    "    b2 = b2 - lr*db2\n",
    "\n",
    "    loss.append( -(1.0/m)*np.sum( Y*np.log(A2) + (1.0-Y)*np.log( 1.0 - A2)) )\n",
    "    #print(i, loss[-1])\n",
    "  print(f'La función de costo en la iteración {epochs} es {loss[-1]}')\n",
    "  return {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'loss':loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yFZ9yjru9Rc"
   },
   "source": [
    "Ejecutamos el entrenamiento de nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaI3W6vZvB7d"
   },
   "outputs": [],
   "source": [
    "nh1=200\n",
    "result=shallow_NN( Training_data, Clas, 1, 500, nh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJLhZUAavCuz"
   },
   "source": [
    "Pedimos información acerca de la evolución de la función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RimjEmbvK6y"
   },
   "outputs": [],
   "source": [
    "plt.plot(result['loss'], label='loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfT4O9PmvOWr"
   },
   "source": [
    "### Predicciones de los datos de prueba\n",
    "Generamos las predicciones de nuestros datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ78agTivPDM"
   },
   "outputs": [],
   "source": [
    "def predict( results, X ):\n",
    "\n",
    "  A0 = X\n",
    "  W1 = results['W1']\n",
    "  b1 = results['b1']\n",
    "\n",
    "  W2 = results['W2']\n",
    "  b2 = results['b2']\n",
    "\n",
    "  # Forward propagation\n",
    "  Z1 = W1 @ A0 + b1\n",
    "  A1 = np.tanh(Z1)\n",
    "\n",
    "  Z2 = W2 @ A1 + b2\n",
    "  A2 = ( 1.0 + np.exp(-Z2) )**(-1)\n",
    "\n",
    "  #yhat = np.around(A2)\n",
    "  return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JpelrwGvUyO"
   },
   "source": [
    "Hacemos las predicciones de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUoNncPzvW4-"
   },
   "outputs": [],
   "source": [
    "y_test_hat = np.around(predict(result, Test))\n",
    "Bayes_test_hat=np.around(Bayes(np.transpose(TestX),np.transpose(TestY)))\n",
    "print(y_test_hat)\n",
    "print(Bayes_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOanD9K7vb5Z"
   },
   "source": [
    "Calculamos el porcentaje de error de la red como del clasificador de la regla de Bayes. Comparamos esas proporciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGOMsnOqvdXm"
   },
   "outputs": [],
   "source": [
    "#Vector de errores para ambos clasificadores (si la diferencia es 0, entonces el valor se está clasificando correctamente, en caso contrario dará 1,-1)\n",
    "ERROR_NN=np.abs(y_test_hat-Etiquetas)\n",
    "ERROR_BAYES=np.abs(Bayes_test_hat-Etiquetas)\n",
    "#Cuantificamos la cantidad de Errores cometidos entre la cantidad de datos de prueba 2m\n",
    "print(f'La proporción de error de clasificación es {np.sum(ERROR_NN)/(2*m)} para la red neuronal')\n",
    "print(f'La proporción de error de clasificación es {np.sum(ERROR_BAYES)/(2*m)} para la regla de Bayes')\n",
    "#Calculamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDo6p3fcwHhj"
   },
   "source": [
    "### Graficando el clasificador generado por la red neuronal\n",
    "Graficamos los puntos pruebas y como fueron clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC0cwTYCwHF3"
   },
   "outputs": [],
   "source": [
    "# Graficamos los puntos\n",
    "\n",
    "#Primero separamos los datos como los clasifico la red neuronal\n",
    "x_T0=Test[0,y_test_hat[0,:]==0]\n",
    "y_T0=Test[1,y_test_hat[0,:]==0]\n",
    "x_T1=Test[0,y_test_hat[0,:]==1]\n",
    "y_T1=Test[1,y_test_hat[0,:]==1]\n",
    "\n",
    "#Los datos clasificados realmente\n",
    "x_R0=test0[:,0]\n",
    "y_R0=test0[:,1]\n",
    "x_R1=test1[:,0]\n",
    "y_R1=test1[:,1]\n",
    "\n",
    "#Generamos la regla de clasificación dada por la red neuronal\n",
    "Q=np.array((Mx,My)) #2x100x100\n",
    "A=np.zeros((100,100))\n",
    "#Generamos la información de la superficie generada por la red sobre la malla\n",
    "for i in range(100):\n",
    "  for j in range(100):\n",
    "    S=np.expand_dims(Q[:,i,j],axis=1)\n",
    "    A[i,j]=predict(result, S)\n",
    "\n",
    "#Cortamos para obtener la curva de nivel 0.5\n",
    "cn2=plt.contour(Mx,My,A, levels=1, colors='red', linestyles= \"dashed\")\n",
    "plt.clabel(cn2, cn2.levels, inline = True, fontsize = 8)\n",
    "\n",
    "# Graficamos los datos clasificados según la red\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "plt.scatter(x_T0, y_T0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.scatter(x_T1, y_T1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.title(f'Datos clasificados por la red con {nh1} neuronas')\n",
    "\n",
    "#Función de separación optima obtenida mediante la distribución de los datos\n",
    "cnt=plt.contour(Mx,My,P, levels=1, colors='black')\n",
    "plt.clabel(cnt, cnt.levels, inline = True, fontsize = 8)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Figura de los datos aleatorios con su etiqueta real\n",
    "plt.xlim(-12,12)\n",
    "plt.ylim(-12,12)\n",
    "plt.scatter(x_R0, y_R0, alpha=0.5, color='teal', label='Clase 0')\n",
    "plt.scatter(x_R1, y_R1, alpha=0.5, color='orange', label='Clase 1')\n",
    "plt.title(f'Clasificador por una red con {nh1} neuronas ocultas')\n",
    "\n",
    "#Función de separación obtenida mediante la red\n",
    "cn2=plt.contour(Mx,My,A, levels=1, colors='red', linestyles= \"dashed\")\n",
    "plt.clabel(cn2, cn2.levels, inline = True, fontsize = 10)\n",
    "#Función de separación optima obtenida mediante la distribución de los datos\n",
    "cnt=plt.contour(Mx,My,P, levels=1, colors='black')\n",
    "plt.clabel(cnt, cnt.levels, inline = True, fontsize = 10)\n",
    "plt.annotate('Clasificador de Bayes', (5,-6), fontsize=8, color='black')\n",
    "plt.annotate('Clasificador de la Red', (5,5), fontsize=8, color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('mi_grafico1.png', dpi=300, bbox_inches='tight')\n",
    "files.download('mi_grafico1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E6CJuXmwWwa"
   },
   "source": [
    "### Comparación de resultados cambiando el número de neuronas\n",
    "\n",
    "En esta sección ejecutamos todos los códigos anteriores variando las neuronas en la capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJxMalucwXkw"
   },
   "outputs": [],
   "source": [
    "#Damos un vector donde cada entrada es el número de neuronas en la capa\n",
    "Neurons=[1,2,3,4,5,10,20,50,100,200,1000]\n",
    "\n",
    "#Ejecutamos para cada cantidad de neuronas\n",
    "for t in Neurons:\n",
    "  result=shallow_NN( Training_data, Clas, 1, 200, nh1=t)\n",
    "  #Proporción de error\n",
    "  y_test_hat = np.around(predict(result, Test))\n",
    "  #Vector de errores para ambos clasificadores (si la diferencia es 0, entonces el valor se está clasificando correctamente, en caso contrario dará 1,-1)\n",
    "  ERROR_NN=np.abs(y_test_hat-Etiquetas)\n",
    "  ERROR_BAYES=np.abs(Bayes_test_hat-Etiquetas)\n",
    "  #Cuantificamos la cantidad de Errores cometidos entre la cantidad de datos de prueba 2m\n",
    "  print(f'La proporción de error de clasificación es {np.sum(ERROR_NN)/(2*m)} para la red neuronal')\n",
    "  print(f'La proporción de error de clasificación es {np.sum(ERROR_BAYES)/(2*m)} para la regla de Bayes')\n",
    "\n",
    "  #Graficamos los datos reales con su etiqueta real\n",
    "  plt.xlim(-12,12)\n",
    "  plt.ylim(-12,12)\n",
    "  plt.scatter(x_R0, y_R0, alpha=0.5, color='black', label='Clase 0')\n",
    "  plt.scatter(x_R1, y_R1, alpha=0.5, color='red', label='Clase 1')\n",
    "  plt.title('Datos con clasificación real')\n",
    "  #Por cada iteración determinamos el porcentaje de error de clasificación\n",
    "  y_test_hat = np.around(predict(result, Test))\n",
    "  ERROR=np.abs(y_test_hat-Etiquetas) #Vector de errores (si la diferencia es 0, entonces el valor se está clasificando correctamente, en caso contrario dará 1,-1)\n",
    "  print(f'La proporción de error de clasificación es {np.sum(ERROR)/(2*m)}') #Cuantificamos la cantidad de Errores cometidos entre la cantidad de datos de prueba 2m\n",
    "\n",
    "  #Por cada iteración calculamos la regla de decisión de la red neuronal\n",
    "  Q=np.array((Mx,My)) #2x100x100\n",
    "  A=np.zeros((100,100))\n",
    "  #Graficamos la superficie\n",
    "  for i in range(100):\n",
    "    for j in range(100):\n",
    "      S=np.expand_dims(Q[:,i,j],axis=1)\n",
    "      A[i,j]=predict(result, S)\n",
    "\n",
    "  #Cortamos para obtener la curva de nivel 0.5\n",
    "  cn2=plt.contour(Mx,My,A, levels=1, colors='green', linestyles= \"dashed\")\n",
    "  plt.clabel(cn2, cn2.levels, inline = False, fontsize = 8)\n",
    "  plt.title(f'Regla de decisión con {t} neuronas')\n",
    "\n",
    "  cnt=plt.contour(Mx,My,P, levels=1)\n",
    "  plt.clabel(cnt, cnt.levels, inline = False, fontsize = 8)\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  plt.contourf(Mx,My,P, levels=1, alpha=0.4)\n",
    "  plt.contour(Mx,My,A, levels=1, colors='black', linestyles= \"dashed\")\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
